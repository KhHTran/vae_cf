{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd03a488e9ece068f6e3d6eaa48bfb08c1d324979f80616fc3050d84a8cc35f6b4b",
   "display_name": "Python 3.8.8 64-bit ('vae_cf': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "import sys\n",
    "import numpy as np \n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn \n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F \n",
    "from tqdm import tqdm\n",
    "import bottleneck as bn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1) # top k\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0)\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis], idx_topk] * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(int(n), k)]).sum() for n in heldout_batch.sum(axis=1)])\n",
    "    return DCG / IDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    userId  movieId  rating   timestamp\n",
       "6        1      151     4.0  1094785734\n",
       "7        1      223     4.0  1112485573\n",
       "8        1      253     4.0  1112484940\n",
       "9        1      260     4.0  1112484826\n",
       "10       1      293     4.0  1112484703"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>151</td>\n      <td>4.0</td>\n      <td>1094785734</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>223</td>\n      <td>4.0</td>\n      <td>1112485573</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>253</td>\n      <td>4.0</td>\n      <td>1112484940</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>260</td>\n      <td>4.0</td>\n      <td>1112484826</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>293</td>\n      <td>4.0</td>\n      <td>1112484703</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "DATA_DIR = '/home/hieutk/Recommend/VAE_CF/ml-20m'\n",
    "\n",
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'), header=0)\n",
    "\n",
    "raw_data = raw_data[raw_data['rating'] > 3.5]\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id) :\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0) :\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users.\n",
    "    if min_sc > 0 :\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount['size'] >= min_sc])]\n",
    "    \n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0 :\n",
    "        usercount = get_count(tp, 'userId') \n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount['size'] >= min_uc])]\n",
    "    \n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId')\n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.003498014804813546\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. * raw_data.shape[0] / user_activity.shape[0] / item_popularity.shape[0]\n",
    "\n",
    "print(sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/test/val split\n",
    "\n",
    "n_users = unique_uid.size\n",
    "n_heldout_users = 10000\n",
    "\n",
    "train_idx = n_users - n_heldout_users * 2\n",
    "test_idx = train_idx + n_heldout_users\n",
    "\n",
    "train_u = uid[: train_idx]\n",
    "test_u = uid[train_idx : test_idx]\n",
    "val_u = uid[test_idx:]\n",
    "\n",
    "train_data = raw_data.loc[raw_data['userId'].isin(train_u)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iid = pd.unique(train_data['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "item2id = dict((iid, idx) for (idx, iid) in enumerate(unique_iid))\n",
    "user2id = dict((uid, idx) for (idx, uid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_iid.txt'), 'w') as f:\n",
    "    for iid in unique_iid:\n",
    "        f.write('%s\\n' % iid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, prop=0.2) :\n",
    "    data_groubyuser = data.groupby('userId')\n",
    "    train = []\n",
    "    test = []\n",
    "\n",
    "    for i, (_, group) in enumerate(data_groubyuser) :\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5 :\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            count = int(n_items_u * prop)\n",
    "            idx[np.random.choice(n_items_u, size=count, replace=False).astype('int64')] = True\n",
    "\n",
    "            train.append(group[np.logical_not(idx)])\n",
    "            test.append(group[idx])\n",
    "        else :\n",
    "            train.append(group)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "    data_train = pd.concat(train)\n",
    "    data_test = pd.concat(test)\n",
    "        \n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = raw_data.loc[raw_data['userId'].isin(val_u)]\n",
    "val_data = val_data.loc[val_data['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "val_train, val_test = split_train_test(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "test_data = raw_data.loc[raw_data['userId'].isin(test_u)]\n",
    "test_data = test_data.loc[test_data['movieId'].isin(unique_sid)]\n",
    "\n",
    "test_train, test_test = split_train_test(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize(df) :\n",
    "    uid = list(map(lambda x: user2id[x], df['userId']))\n",
    "    iid = list(map(lambda x: item2id[x], df['movieId']))\n",
    "    return pd.DataFrame(data={'uid':uid, 'iid':iid}, columns=['uid','iid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ = numerize(train_data)\n",
    "train_data_.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
    "\n",
    "val_tr = numerize(val_train)\n",
    "val_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
    "\n",
    "val_te = numerize(val_test)\n",
    "val_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
    "\n",
    "test_tr = numerize(test_train)\n",
    "test_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
    "\n",
    "test_te = numerize(test_test)\n",
    "test_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "    def __init__(self, dropout=0.5, q_dims=[20108, 600, 200]) :\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout, in_place=False)\n",
    "        self.q_dims = q_dims\n",
    "        self.fc1 = nn.Linear(self.q_dims[0], self.q_dims[1], bias=True)\n",
    "        self.fc2 = nn.Linear(self.q_dims[1], 2*self,q_dims[2], bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def foward(self, x) :\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        mu, logvar = torch.chunk(x, chunks=2, dim=1)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, p_dims=[200, 600, 20108]) :\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.p_dims = p_dims\n",
    "        self.fc1 = nn.Linear(self.p_dims[0], self.p_dims[1], bias=True)\n",
    "        self.fc2 = nn.Linear(self.p_dims[1], self,p_dims[2], bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def foward(self, x) :\n",
    "        x = self.fc1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVAE(nn.Module) :\n",
    "    def __init__(self, cuda, weight_decay=0.0, dropout=0.5, q_dims=[20108,600,200], p_dims=[200,600,20108], n_conditioned=0) :\n",
    "        assert q_dims[0] == p_dims[-1] and q_dims[-1] == p_dims[0], 'dimmension of AE not match'\n",
    "\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_conditioned = n_conditioned\n",
    "        self.q_dims = q_dims\n",
    "        self.p_dims = p_dims\n",
    "        self.q_dims[0] += self.n_conditioned\n",
    "        self.p_dims[0] += self.n_conditioned\n",
    "\n",
    "        self.enc = Encoder(dropout=dropout, q_dims=self.q_dims)\n",
    "        self.dec = Decoder(p_dims=self.p_dims)\n",
    "    \n",
    "    def forward(self, x, c) :\n",
    "        x = f.normalise(x, p=2, dim=1)\n",
    "        if self.n_conditioned > 0 :\n",
    "            x = torch.cat((x,c), dim=1)\n",
    "        \n",
    "        mu_q, logvar_q = self.enc(x)\n",
    "        std_q = torch.exp(0.5 * logvar_q)\n",
    "        kl = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1), dim=1))\n",
    "        epsilon = torch.randn_like(std_q, requres_grad=False)\n",
    "\n",
    "        if True :\n",
    "            if self.training : \n",
    "                sampled_z = mu_q + epsilon * std_q\n",
    "            else :\n",
    "                sampled_z = mu_q\n",
    "        else :\n",
    "            sampled_z = mu_q + epsilon * std_q\n",
    "        \n",
    "        if self.n_conditioned > 0 :\n",
    "            sampled_z = torch.cat((sampled_z, c), dim=1)\n",
    "        logits = self.dec(sampled_z)\n",
    "        \n",
    "        return logits, kl, mu_q, std_q, epsilon, sampled_z\n",
    "    \n",
    "    def get_l2_reg(self) :\n",
    "        l2_reg = torch.autograd.Variable(torch.FloatTensor(1), requires_grad=True)\n",
    "        \n",
    "        if self.weight_decay > 0 :\n",
    "            for k, m in self.state_dict().items() :\n",
    "                if k.endswith('.weight') :\n",
    "                    l2_reg = l2_reg + torch.norm(m, p=2)**2\n",
    "        if self.cuda :\n",
    "            l2_reg = l2_reg.cuda()\n",
    "        return self.weight_decay * l2_reg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object) :\n",
    "    def __init__(\n",
    "        self, model, cuda, optimizer, train_loader, \n",
    "        test_loader, val_loader, interval_valid, \n",
    "        anneal_step=2e5, anneal_cap=0.2\n",
    "    ) :\n",
    "        super(Trainer, self).__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.cuda = cuda\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.interval_valid = interval_valid\n",
    "        self.step = 0\n",
    "        self.anneal_step = anneal_step\n",
    "        self.anneal_cap = anneal_cap\n",
    "        self.n20_all = []\n",
    "        self.n20_max_va, self.n100_max_va, self.r20_max_va, self.r50_max_va = 0, 0, 0, 0\n",
    "        self.n20_max_te, self.n100_max_te, self.r20_max_te, self.r50_max_te = 0, 0, 0, 0\n",
    "\n",
    "    #training epoch:\n",
    "    def train_epoch(epoch) :\n",
    "        #mark model as training\n",
    "        self.model.train()\n",
    "        \n",
    "        for batch_id, (data_tr, data_te, prof) in tqdm(enumerate(self.train_loader)) :\n",
    "            self.step += 1\n",
    "\n",
    "            if self.cuda :\n",
    "                data_tr = data_tr.cuda()\n",
    "                prof = prof.cuda()\n",
    "            \n",
    "            data_tr = torch.autograd.Variable(data_tr)\n",
    "            prof = torch.autograd.Variable(prof)\n",
    "\n",
    "            logits, KL, mu_q, std_q, epsilon, sampled_z = self.model(data_tr, prof)\n",
    "            log_softmax = f.log_softmax(logits, dim=1)\n",
    "            neg_ll = torch.mean(torch.sum(log_softmax * data_tr, dim=1))\n",
    "            l2_reg = self.model.get_l2_reg()\n",
    "\n",
    "            if self.anneal_step > 0 :\n",
    "                anneal = min(self.anneal_cap, 1. * self.step / self.anneal_step)\n",
    "            else :\n",
    "                anneal = self.anneal_cap\n",
    "            \n",
    "            loss = neg_ll + anneal * kl + l2_reg\n",
    "            \n",
    "            print(epoch, batch_idx, loss.item(), anneal, self.step, self.optimizer.param_groups[0]['lr'])\n",
    "            print(neg_ll.cpu().detach().numpy(), KL.cpu().detach().numpy(), l2_reg.cpu().detach().numpy()/2)\n",
    "            print('==================================')\n",
    "\n",
    "            #backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if self.interval_valid > 0 and (self.step + 1) % self.interval_valid == 0 :\n",
    "                print(\"CALLING VALID train \", self.step)\n",
    "                validate(epoch)\n",
    "    \n",
    "    def validate(epoch, cmd='valid') :\n",
    "        # mark model as evaluating\n",
    "        self.model.eval()\n",
    "        \n",
    "        if cmd == 'valid' :\n",
    "            loader = self.val_loader\n",
    "        else :\n",
    "            loader = self.test_loader\n",
    "\n",
    "        n20, n100, r20, r100 = [], [], [], []\n",
    "\n",
    "        for batch_id, (data_tr, data_te, prof) in tqdm(enumerate(loader)) :\n",
    "            \n",
    "            if self.cuda :\n",
    "                data_tr = data_tr.cuda()\n",
    "                prof = prof.cuda()\n",
    "            \n",
    "            data_tr = torch.autograd.Variable(data_tr)\n",
    "            prof = torch.autograd.Variable(prof)\n",
    "\n",
    "            with torch.no_grad() :\n",
    "                logits, KL, mu_q, std_q, epsilon, sampled_z = self.model(data_tr, prof)\n",
    "\n",
    "                pred_val = logits.cpu().detach().numpy()\n",
    "                pred_val[data_tr.cpu().detach().numpy().nonzero()] = -np.inf\n",
    "                \n",
    "                n20.append(utils.NDCG_binary_at_k_batch(pred_val, data_te.numpy(), k=20))\n",
    "                n100.append(utils.NDCG_binary_at_k_batch(pred_val, data_te.numpy(), k=100))\n",
    "                r20.append(utils.Recall_at_k_batch(pred_val, data_te.numpy(), k=20))\n",
    "                r50.append(utils.Recall_at_k_batch(pred_val, data_te.numpy(), k=50))\n",
    "        \n",
    "        n20 = np.concatenate(n20, axis=0)\n",
    "        n100 = np.concatenate(n100, axis=0)\n",
    "        r20 = np.concatenate(r20, axis=0)\n",
    "        r50 = np.concatenate(r50, axis=0)\n",
    "\n",
    "        if cmd == 'valid' :\n",
    "            self.n20_max_va = max(self.n20_max_va, n20.mean())\n",
    "            self.n100_max_va = max(self.n100_max_va, n100.mean())\n",
    "            self.r20_max_va = max(self.r20_max_va, r20.mean())\n",
    "            self.r50_max_va = max(self.r50_max_va, r50.mean())\n",
    "            max_metrics = \"{},{},{},{:.5f},{:.5f},{:.5f},{:.5f}\".format(\n",
    "                cmd, epoch, self.step, \n",
    "                self.n20_max_va, self.n100_max_va, \n",
    "                self.r20_max_va, self.r50_max_va\n",
    "            )\n",
    "        else :\n",
    "            self.n20_max_te = max(self.n20_max_te, n20.mean())\n",
    "            self.n100_max_te = max(self.n100_max_te, n100.mean())\n",
    "            self.r20_max_te = max(self.r20_max_te, r20.mean())\n",
    "            self.r50_max_te = max(self.r50_max_te, r50.mean())\n",
    "            max_metrics = \"{},{},{},{:.5f},{:.5f},{:.5f},{:.5f}\".format(\n",
    "                cmd, epoch, self.step, \n",
    "                self.n20_max_te, self.n100_max_te, \n",
    "                self.r20_max_te, self.r50_max_te\n",
    "            )\n",
    "\n",
    "        metrics = []\n",
    "        metrics.append(max_metrics)\n",
    "        metrics.append(\"NDCG@20,{:.5f},{:.5f}\".format(np.mean(n20), np.std(n20) / np.sqrt(len(n20))))\n",
    "        metrics.append(\"NDCG@100,{:.5f},{:.5f}\".format(np.mean(n100), np.std(n100) / np.sqrt(len(n100))))\n",
    "        metrics.append(\"Recall@20,{:.5f},{:.5f}\".format(np.mean(r20_list), np.std(r20) / np.sqrt(len(r20))))\n",
    "        metrics.append(\"Recall@50,{:.5f},{:.5f}\".format(np.mean(r50_list), np.std(r50) / np.sqrt(len(r50))))\n",
    "        print('\\n' + \",\".join(metrics))\n",
    "\n",
    "        # return model to training\n",
    "        self.model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}